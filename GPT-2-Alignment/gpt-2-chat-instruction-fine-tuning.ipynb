{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -U transformers datasets peft trl bitsandbytes --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UEk-anDZ1my",
        "outputId": "d44ff01c-3eb6-4b9b-d57f-bafa225d8ac8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mqa3wQrER4Mu",
        "outputId": "25babd50-b629-4bf4-fd53-813fc8d99792"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mbv5vY7-FSuV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "a7e34657ae5d497fba9df4edaf5c7a1d",
            "e02c49e15e8d4e91aca4b360b48081b4",
            "a1607f31466d4313978cf65831f02588",
            "f83ea18f974f4396a0a4ccbbf622032e",
            "50ce49f5fb524242b8f20611e1e5c2b1",
            "40c6bec956014b5a938fe342ca51aa03",
            "dee917c0f3a44249be5cc8da04cf1dbc",
            "0db8afffa4784742800297624a79231a",
            "3d227b145d8948648a1058671baa43eb",
            "440aefa64b0f4419aea76b9c052b16db",
            "b5fdc1e6fdbc4abfb4333ac9e3dbb224"
          ]
        },
        "outputId": "61d8a801-087b-4d68-c713-01d739f3eeba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7e34657ae5d497fba9df4edaf5c7a1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"openai-community/gpt2-medium\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_template(instruction: str, output: str | None = None) -> str:\n",
        "    user_tag = \"<|User|>\"\n",
        "    assistant_tag = \"<|Assistant|>\"\n",
        "    prompt = f\"{user_tag}\\n{instruction}\\n{assistant_tag}\\n\"\n",
        "\n",
        "    if output:\n",
        "        prompt += output\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "RZ_sqZVSSQdK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "text = \"What is the president of the United States doing all day?\"\n",
        "\n",
        "pprint(chat_template(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWubnFpqTKSa",
        "outputId": "2480b1a7-74ee-4b23-dc7b-ae7c5e5663f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('<|User|>\\n'\n",
            " 'What is the president of the United States doing all day?\\n'\n",
            " '<|Assistant|>\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer(chat_template(text), return_tensors=\"pt\")\n",
        "token_ids = {k: v.to(device) for k, v in token_ids.items()}\n",
        "token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi49v59VNCvP",
        "outputId": "cafbaf5d-b535-4db5-a2c4-dc5232d2f7b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[   27,    91, 12982,    91,    29,   198,  2061,   318,   262,  1893,\n",
              "            286,   262,  1578,  1829,  1804,   477,  1110,    30,   198,    27,\n",
              "             91, 48902,    91,    29,   198]], device='cuda:0'),\n",
              " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1]], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**token_ids, max_new_tokens=50, repetition_penalty=1.2, do_sample=True, temperature=0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BHWP0MVN1V0",
        "outputId": "1ecf38b6-4a2a-4639-cbe8-f26d00710b36"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKP5M407OiGv",
        "outputId": "e68c3b21-38ba-4099-c935-0c4d92ea5cc2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('<|User|>\\n'\n",
            " 'What is the president of the United States doing all day?\\n'\n",
            " '<|Assistant|>\\n'\n",
            " \"So when do we start seeing signs, like this one: 'We're not going to let you \"\n",
            " \"out until our troops are safe?' (or something similar). What does that mean \"\n",
            " 'in your opinion and how can everyone avoid it without being a danger '\n",
            " 'yourself')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat-Instruction Fine Tuning"
      ],
      "metadata": {
        "id": "x70LW8-qVSQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\", streaming=True)"
      ],
      "metadata": {
        "id": "mB0AlanvWYn3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_empty_input(sample):\n",
        "    return sample[\"input\"] == \"\"\n",
        "\n",
        "filtered_dataset = dataset.filter(filter_empty_input)"
      ],
      "metadata": {
        "id": "oPGWkpfxXaEG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _tokenize(sample):\n",
        "    prompt = f\"<|User|>\\n{sample['instruction']}\\n\\n<|Assistant|>\\n{sample['output']}{tokenizer.eos_token}\"\n",
        "    return tokenizer(prompt, truncation=True, max_length=512)\n",
        "\n",
        "tokenized_stream = filtered_dataset.map(\n",
        "    _tokenize,\n",
        "    remove_columns=list(filtered_dataset.features)\n",
        ")"
      ],
      "metadata": {
        "id": "rCmmcZzCY5GO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_sft-chat\",\n",
        "    max_steps=500,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    optim=\"adamw_8bit\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_stream,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5824e67c899b4dbaa635e5cbe2255c7f",
            "33b366bcb4dd436e81e9bfb5e9a210e8",
            "58556d28c1e94bb5aa16bc7de3073c57",
            "39114c03c30f422ea767c0106b5d7c24",
            "00503234cd494c80997f95510cbc14c4",
            "6e1170db7cde42af8fd5e2c61c8b89ef",
            "c3d4d5c00c7e40e583c2c30485d0695c",
            "5d1a8794b03c43d58281d2096e508b93",
            "abaeb562525244e5860c59074a144982",
            "256d117cd05c4df396fc1b47a28b7af5",
            "4875041cc3894fb5929123f0acad809e"
          ]
        },
        "id": "HH8SvENPbcVU",
        "outputId": "5c9f75ca-ecc4-4b80-8fcc-12b634853aac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 18:01, Epoch 1/9223372036854775807]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.152925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.005300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.000811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.921167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.991450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.906825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.947467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.882538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.884331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.833370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.873914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.861600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.851664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.887257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.833092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.805433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.730420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.913651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.864698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.933604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.935403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.789671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.786735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.816975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.820506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.850281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.764121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.767641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.765384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.771813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.750158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.724277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.722904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.770856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.786169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.708638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.733110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.726437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.726712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.874946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>1.685002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>1.732527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>1.786913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.775210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.737785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.670829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>1.705052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.760861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>1.724134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.807678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5824e67c899b4dbaa635e5cbe2255c7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=1.8212048683166504, metrics={'train_runtime': 1086.8602, 'train_samples_per_second': 3.68, 'train_steps_per_second': 0.46, 'total_flos': 2014972149473280.0, 'train_loss': 1.8212048683166504, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "2pCHhSE3el09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36tAqcOKdPr0",
        "outputId": "ddc011c2-f80e-487f-fdf7-3439fb5624bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-23): 24 x GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
              "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
              "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "text = \"What is the president of the United States doing all day?\"\n",
        "\n",
        "prompt = chat_template(text)\n",
        "token_ids = tokenizer(chat_template(text), return_tensors=\"pt\")\n",
        "token_ids = {k: v.to(device) for k, v in token_ids.items()}\n",
        "\n",
        "outputs = model.generate(**token_ids, max_new_tokens=50, repetition_penalty=1.2, do_sample=True, temperature=0.7)\n",
        "pprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbtKYLZSiGpq",
        "outputId": "8638eb15-0d27-4b96-c823-336275c923be"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('<|User|>\\n'\n",
            " 'What is the president of the United States doing all day?\\n'\n",
            " '<|Assistant|>\\n'\n",
            " 'The President works from the White House in the morning to conduct official '\n",
            " 'business, including meetings with his Cabinet members and legislative '\n",
            " 'representatives. He then begins working at the West Wing where he meets '\n",
            " 'regularly for breakfast on the South Lawn before taking off to continue the '\n",
            " 'day')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "90a3239d3f76434d83eaadf4b8141d57",
            "a4863d70bb8e4a98aac3cab756eeee2e",
            "ee63bb938fa64505b31bb85f138b89ca",
            "015f676d97274027b368b404ca51fed1",
            "1c17577c3bda49a1843eb54196191e3f",
            "56b7268a0b6543d1aaeb17d92f3a24bf",
            "55b3cc55f0974fa38c6ada7c1f573c5d",
            "8acd2e5f14f34709bb72458888f16c70",
            "35234647113a4a01968888085a67c794",
            "ae82ad3dbece48e0984402618e4d7b16",
            "0b9860db9646454db710d40f2a8b7c85",
            "62e278302e4749658ad5f5bacddb70db",
            "c8c3cdfb77ea491cae209a83561c29cb",
            "3a65087eb35f48c188245e47315ff0a1",
            "84f2eaf65a5f483891fa6cf9b2048b4b",
            "d4826745b7294b2d815ac2091a87c3dc",
            "4f7c6f4301574b77bf06cc9c6e8df6e3",
            "8afa8c02f68e4d9798f57a5c2d7f4f5a",
            "89025359c12f466b958d2df941fcef5d",
            "f006d1c7d311404cb2303433cda13b74"
          ]
        },
        "id": "N-jFWB5rieiQ",
        "outputId": "3d0b5552-eb64-4818-8645-e03de69d9f50"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90a3239d3f76434d83eaadf4b8141d57"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"CKeibel/gpt2-medium-chat\"\n",
        "tokenizer.push_to_hub(repo_id)\n",
        "trainer.model.push_to_hub(repo_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246,
          "referenced_widgets": [
            "e7aeaf13990a49b9a945c1a2e9d20410",
            "2672a0e5d67b4eb7b55f23c6cf6ca48f",
            "4ac79acf68d04d7e83af56b00c9e9d96",
            "8ecc00329ce44171a1a7a411fc714bb3",
            "6784d719823b462a830615926d4c0966",
            "5cb8901719f44f83869b5327e33556c3",
            "51c1dffcd1c646d283aabf7d3b33ab1d",
            "bfbbba54dc8240219770ac6f5f4cad0c",
            "6c24a42edc9747d3b11a5438d996a4fa",
            "6c801539be7f4469b42c301723c967c8",
            "54a33518995e414dae95c4414df7aa38",
            "a58aac2757f6483db50f7675ebc9b476",
            "d3f66f2a7e03454b8f114ff0df8003cb",
            "2fb2d18e9c264e418fe7ca32bff96b19",
            "3d2b7093ef6843b0b0df9d1232490cee",
            "2b60ffe60481494588a5c38dc5053714",
            "f904d014f0b84b47972b55936a766af7",
            "6664d28fee814930b7ef15cf06a9292a",
            "67f6b0a82ed04c13b03ac0942d37d638",
            "e86f048f45c84f62ba43e0de902907b5",
            "e586580d97234268a2573faf3b38fdc1",
            "f477cdab5c4f4c0ea4de444aa2e38b65",
            "359cd199407a499cb9971887055c5ca4",
            "d7a067384cf94b33932dbe1173602a31",
            "ed5e55bae58f46789971875bc1240cb0",
            "2d3e16ed178f4d6a9c837bdc49fde00e",
            "b7c89fbf419a4d6684922c076d2cca6f",
            "1098b83592604e169bd2ec85853770c4",
            "481870c6490b40329da7a6ccc02157ba",
            "0e90232ea307405eb775d5c13f20da94",
            "7c00f965d3724226b5c3b963c4ce32b0",
            "2a68dc99667d414bb023a923555b2a56",
            "a98debb710f04367871396e507646f11",
            "b79cdec8af1a4423bab25d48d40cc546",
            "ece01f447bf9439abea12a2e27e5a945",
            "7e099bca6a244e25bcb531bee7059a82",
            "92cc87c5e6c246318f32bcc4aedc8cfc",
            "099c73d234fc40f9801f656f82bf6e5c",
            "57dc3e477b574f36b0aebe556cdaecb1",
            "75a1abb31d1b417db525b990f0d1e3ef",
            "486c82e765c243ebbed0d67deb2a1304",
            "e99316e531da41ceab34b4b70d5979f5",
            "8d54d61a372e43a3813a29960312f18a",
            "3220779e10da48c4bb316335f792c904",
            "cddb4df477534f62bf9521f57346fd30",
            "0fd7281417c1481890ff7f9a055ee1d4",
            "50a9d184b0d543738b98a6f4fd50eb38",
            "5c1bb0c517a543d2a7aad3d3a4e289f4",
            "4f4b74fbc6664241988524aa181060bc",
            "ea8a15bc4d8942aeb4a6a2f1ebc25db7",
            "7860a794f2fc43b3bbc54d2f8fedd450",
            "5f69a0e6d3004bbfa4deb17cfdd7b1bd",
            "9b5511f6800d438eb5ee1c1677ee4784",
            "bc0e5939633046549ac46b63df209e3b",
            "68e9eea1b3404efeb272a3778d6e03f8"
          ]
        },
        "id": "WGdBT4AwjY3E",
        "outputId": "d026b3e5-056e-4554-fd78-a2ac0931876e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7aeaf13990a49b9a945c1a2e9d20410"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a58aac2757f6483db50f7675ebc9b476"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "359cd199407a499cb9971887055c5ca4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b79cdec8af1a4423bab25d48d40cc546"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...z3q0ha9/model.safetensors:   0%|          |  549kB / 1.42GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cddb4df477534f62bf9521f57346fd30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/CKeibel/gpt2-medium-chat/commit/539685c5b65d244bd5ab825fd700b265e02dd415', commit_message='Upload model', commit_description='', oid='539685c5b65d244bd5ab825fd700b265e02dd415', pr_url=None, repo_url=RepoUrl('https://huggingface.co/CKeibel/gpt2-medium-chat', endpoint='https://huggingface.co', repo_type='model', repo_id='CKeibel/gpt2-medium-chat'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}