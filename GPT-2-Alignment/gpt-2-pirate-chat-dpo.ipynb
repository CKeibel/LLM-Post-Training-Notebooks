{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -U trl transformers peft bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "402r0OaEOAMh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G-1jRGbWOeGT",
        "outputId": "75609d9a-df18-439e-8050-b8f6dcd31385"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "1bf0de528d9c4fe9a94f81f571302fa2",
            "7308d35d048844198e18c2b8a59d2f70",
            "db1b4693794d402b8c9f961a133539ea",
            "8281f99ea3094c0e9c4f51e57e2f6afa",
            "7db3e333005d475ba388726f85622666",
            "a01d4e1f91774d9aa556c65f47497f29",
            "1f8b8fbef6cb4b54ba2ccdb9cc55e7b6",
            "2a9a450fb29d4cfa9aaf02c380057088",
            "24be6338cf0340458e63f355c7b54532",
            "76eb5240b6e04a91bae69f8637d3343e",
            "175a04ea219d4cd9840068c219d7aacd"
          ]
        },
        "id": "-fxHLKwBM8Z4",
        "outputId": "36f1025a-dacc-4f8a-85b1-8c6726b12fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bf0de528d9c4fe9a94f81f571302fa2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_id = \"CKeibel/gpt2-medium-chat\"\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is Deep Learning?\"}\n",
        "]\n",
        "\n",
        "output = pipe(messages, max_new_tokens=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAyvKTBvNPdJ",
        "outputId": "f880ec5b-cc73-4670-f199-67b5ace52593"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=50) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[0]['generated_text'][-1]['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt4Q-5CTOLzh",
        "outputId": "a80e9ba2-a055-45be-aa9e-5e3353c8b216"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Learning is a branch of Artificial Intelligence that focuses on the creation of neural networks that can process large amounts of data and make predictions based on it. The key goal of deep learning is to enable computers to learn from data and make predictions based on it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8_-KPB3dOy9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "d281383749b047209f672305a1c4b590",
            "9b84df551929422488b09d973e3c0adb",
            "2ecea5b9dfe4458cb1cd1ca5155732da",
            "edc52c6c77cf42faa5f6c635e7ae14f8",
            "dde42655b6434c8786097b4dcfd09e4d",
            "20e5be904ce24487b17e2bf0f8ab90c5",
            "47cf5676b50340cbadf48c46899777e3",
            "37028db2c6104e569f74cef650d44286",
            "adce981a3fc3421bb4d667852ce28abd",
            "0c972fd424ad407ab5b0b1c79dd1e1de",
            "245fa6e81af941ca8796b09a42184f10"
          ]
        },
        "id": "5okkjEVQOzoR",
        "outputId": "4615f056-1815-43eb-b954-80dd411465bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d281383749b047209f672305a1c4b590"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, LoraConfig\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"c_attn\"]\n",
        ")\n",
        "\n",
        "dpo_config = DPOConfig(\n",
        "    output_dir=\"./gpt2_pirate_dpo\",\n",
        "    beta=0.5,\n",
        "    learning_rate=1e-6,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    max_steps=200,\n",
        "    logging_steps=10,\n",
        "    optim=\"adamw_8bit\",\n",
        "    remove_unused_columns=False\n",
        ")"
      ],
      "metadata": {
        "id": "Ey4cdYRVPFXL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_id = \"CKeibel/synthetic-pirate-alpaca-small\"\n",
        "dataset = load_dataset(dataset_id, split=\"train\")"
      ],
      "metadata": {
        "id": "XOZJr_jRQoWD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,\n",
        "    args=dpo_config,\n",
        "    peft_config=peft_config,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gzjkq9pQcd2",
        "outputId": "9e1d7b5c-d5e0-4cc6-cf94-562c7bce0a1f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "jyZWpVz0Q2LL",
        "outputId": "c050a190-14fa-482a-dc17-f3e2a33a4b44"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 31:08, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.690303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.694232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.690777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.679959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.675151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.656151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.653528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.643239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.634006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.630089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.619731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.618083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.592235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.586357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.584839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.562520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.566949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.564601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.553032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.558486</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=0.6227134132385254, metrics={'train_runtime': 1875.5322, 'train_samples_per_second': 0.853, 'train_steps_per_second': 0.107, 'total_flos': 0.0, 'train_loss': 0.6227134132385254, 'epoch': 3.176})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = trainer.model.merge_and_unload()"
      ],
      "metadata": {
        "id": "ogX8doLwRgE1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is Deep Learning?\"}\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        ")\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = final_model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    repetition_penalty=1.2,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXhf-dDgTG0J",
        "outputId": "bbe2f786-2fe9-4a8f-d60d-ab85e981b719"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep D  ... and as in,,\" \" } // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // // //\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359,
          "referenced_widgets": [
            "5be64efadd66493f930d5f9022db88d5",
            "edc90bb1ab484107b74e1332be90fe2d",
            "7ab0d1f127b94ab1a80841c8a71c5025",
            "1f609f651df44389b00ceadf993dc96c",
            "cb71c0b7d47d41cf86cd6d6fdfc69ae0",
            "ebd6be18a7164ef5a0771320934d4593",
            "0b268fd3024740edb5921157e2f2fb39",
            "832bb445c0304ff5a8bb6acc4e630b1c",
            "9de8213b96ed44e5b2032344179a5a77",
            "f34f2d5e36a24309af64d228e41dc56c",
            "90382c5e6b584c46b9c908842b926d07",
            "ca9eb95969bb44f3a7fccd7d5ed22ff7",
            "6f6a5a2f72ba4cb19fdca30c1907467f",
            "f182edcd83614af8a94788ef0dfc80b8",
            "fb189329b36b49b89e5afb40dff3b7cd",
            "1c0676e222d644d7b0f7c9b28ca170b6",
            "46da31446b504bc5bcfc9a1632127b9b"
          ]
        },
        "id": "Vl8SmFoYTm9n",
        "outputId": "60379361-9e9c-4457-8721-f5baf0b345eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5be64efadd66493f930d5f9022db88d5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.push_to_hub(\"CKeibel/gpt2-medium-chat-pirate-dpo\")"
      ],
      "metadata": {
        "id": "8qUObc7oTqiS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}